# Chapter 4: The Nervous System (Observability)

**Learning Objectives**:
- Understand the three pillars of observability: metrics, logs, and traces
- Design and implement a production-grade observability stack
- Master Prometheus for metrics collection and alerting
- Build effective Grafana dashboards for operational insight
- Implement Loki for centralized log aggregation
- Deploy Jaeger for distributed tracing
- Integrate observability into infrastructure workflows
- Apply the Golden Signals framework to your services
- Troubleshoot production issues using observability data

---

## Introduction: Why Observability Matters

**Monitoring tells you WHAT is broken. Observability tells you WHY.**

Your infrastructure is a living organism. Like a human body, it needs a nervous system to:
- Sense what's happening in every component
- Detect anomalies before they become failures
- Provide feedback for optimization
- Enable rapid diagnosis when problems occur
- Support continuous improvement through data

Without observability, you're operating blindfolded. With it, you have superhuman insight into your infrastructure's health and behavior.

### The Evolution: Monitoring ‚Üí Observability

**Traditional Monitoring**:
```
Known knowns: "Is service X up?"
‚Üí Binary health checks
‚Üí Fixed thresholds
‚Üí Reactive alerting
‚Üí Limited context
```

**Modern Observability**:
```
Unknown unknowns: "Why is service X slow for user Y?"
‚Üí Rich contextual data
‚Üí Dimensional analysis
‚Üí Proactive investigation
‚Üí Full system understanding
```

**The difference**: Monitoring checks if your car's "check engine" light is on. Observability lets you see exactly which cylinder is misfiring and why.

---

## The Three Pillars of Observability

### Pillar 1: Metrics (The Vital Signs) üìä

**What**: Time-series numerical data about system behavior

**Examples**:
- `http_requests_total`: How many requests per second
- `http_request_duration_seconds`: How long requests take
- `node_memory_usage_bytes`: Memory consumption over time
- `postgresql_connections_active`: Database connection pool utilization

**Why metrics matter**:
- Aggregate system behavior into trends
- Enable real-time alerting on thresholds
- Efficient storage (highly compressed time-series)
- Fast queries across millions of data points

**Tool**: Prometheus

### Pillar 2: Logs (The Event Stream) üìù

**What**: Discrete events with context about what happened

**Examples**:
```json
{
  "timestamp": "2024-03-15T10:23:45Z",
  "level": "error",
  "service": "api",
  "message": "Failed to connect to database",
  "error": "connection timeout after 30s",
  "user_id": "12345",
  "request_id": "abc-def-123"
}
```

**Why logs matter**:
- Capture the full context of individual events
- Enable root cause analysis with detailed information
- Support debugging specific user transactions
- Provide audit trails for compliance

**Tool**: Loki

### Pillar 3: Traces (The Journey Map) üîç

**What**: The complete path of a request through your distributed system

**Example**: User clicks "Submit Order"
```
[Browser] --100ms--> [API Gateway] --50ms--> [Order Service]
                                                    |
                                      +-------------+-------------+
                                      |                          |
                                   --30ms-->                  --80ms-->
                              [Payment Service]            [Inventory Service]
                                      |                          |
                                   --20ms-->                  --15ms-->
                                 [Database]                   [Database]
```

**Why traces matter**:
- Understand request flow across microservices
- Identify bottlenecks in distributed systems
- Correlate errors across service boundaries
- Measure end-to-end latency attribution

**Tool**: Jaeger

### The Power of Integration

The three pillars work together:

```
Metrics alert: "API latency increased from 100ms to 2s"
    ‚Üì
Logs reveal: "Database connection pool exhausted"
    ‚Üì
Traces show: "Inventory service query taking 1.8s"
    ‚Üì
Solution: "Optimize inventory query, add index on product_id"
```

**This is the observability feedback loop in action.**

---

## The Golden Signals Framework

Google's Site Reliability Engineering team identified four key metrics that matter for ANY service:

### 1. Latency (Speed) ‚ö°

**Definition**: How long does it take to serve a request?

**Why it matters**: Users notice when things are slow.

**What to measure**:
- Request duration percentiles (p50, p95, p99)
- Separate success vs. error latency
- End-to-end user-perceived latency

**Example metrics**:
```promql
# 95th percentile latency
histogram_quantile(0.95,
  rate(http_request_duration_seconds_bucket[5m])
)

# Slow requests (>1 second)
sum(rate(http_request_duration_seconds_bucket{le="1.0"}[5m]))
```

### 2. Traffic (Volume) üö¶

**Definition**: How much demand is being placed on the system?

**Why it matters**: Capacity planning and anomaly detection.

**What to measure**:
- Requests per second
- Active connections
- Bytes transmitted
- Messages processed

**Example metrics**:
```promql
# Requests per second
rate(http_requests_total[5m])

# Active WebSocket connections
websocket_connections_active

# Database queries per second
rate(postgresql_queries_total[5m])
```

### 3. Errors (Reliability) ‚ùå

**Definition**: What percentage of requests are failing?

**Why it matters**: Errors directly impact user experience.

**What to measure**:
- Error rate (requests/sec)
- Error ratio (percentage)
- Error types and categories
- User-impacting vs. internal errors

**Example metrics**:
```promql
# Error rate
rate(http_requests_total{status=~"5.."}[5m])

# Error percentage
sum(rate(http_requests_total{status=~"5.."}[5m]))
/
sum(rate(http_requests_total[5m])) * 100

# By error type
sum by (error_type) (rate(errors_total[5m]))
```

### 4. Saturation (Capacity) üîã

**Definition**: How "full" is your service?

**Why it matters**: Predict when you'll run out of resources.

**What to measure**:
- CPU utilization (percentage)
- Memory utilization (percentage)
- Disk usage (percentage)
- Connection pool usage (percentage)
- Queue depth (number of items)

**Example metrics**:
```promql
# CPU saturation
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory saturation
(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)
/
node_memory_MemTotal_bytes * 100

# Database connection pool saturation
postgresql_connections_active / postgresql_connections_max * 100
```

### Using the Golden Signals

**Start here for every new service**:
1. Implement these four signal types
2. Create dashboards showing all four
3. Set alerts based on acceptable thresholds
4. Add service-specific metrics later

**Example SLO (Service Level Objective)**:
```yaml
service: api
golden_signals:
  latency:
    p95: < 200ms
    p99: < 500ms
  traffic:
    expected: 1000 req/s
    capacity: 5000 req/s
  errors:
    error_rate: < 0.1%
    error_budget: 99.9% uptime
  saturation:
    cpu: < 70%
    memory: < 80%
    connections: < 80%
```

---

## Prometheus: Metrics Collection and Alerting

Prometheus is the de facto standard for metrics in cloud-native infrastructure.

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Your Services                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ Service A‚îÇ  ‚îÇ Service B‚îÇ  ‚îÇ Service C‚îÇ      ‚îÇ
‚îÇ  ‚îÇ :8080    ‚îÇ  ‚îÇ :8081    ‚îÇ  ‚îÇ :8082    ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ /metrics ‚îÇ  ‚îÇ /metrics ‚îÇ  ‚îÇ /metrics ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ             ‚îÇ             ‚îÇ
        ‚îÇ Pull every 15s (scrape)   ‚îÇ
        ‚îÇ             ‚îÇ             ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   Prometheus   ‚îÇ
              ‚îÇ   (Storage)    ‚îÇ ‚Üê Stores time-series
              ‚îÇ                ‚îÇ ‚Üê Evaluates alert rules
              ‚îÇ   PromQL       ‚îÇ ‚Üê Query language
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ                       ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ  Grafana    ‚îÇ      ‚îÇAlertmanager ‚îÇ
     ‚îÇ (Dashboards)‚îÇ      ‚îÇ   (Alerts)  ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Concepts

**1. Metrics Types**:

```go
// Counter: Only increases (requests, errors)
http_requests_total 157

// Gauge: Can go up or down (memory, connections)
node_memory_usage_bytes 4294967296

// Histogram: Distribution of values (latency)
http_request_duration_seconds_bucket{le="0.1"} 95
http_request_duration_seconds_bucket{le="0.5"} 120
http_request_duration_seconds_bucket{le="1.0"} 125

// Summary: Pre-calculated percentiles
http_request_duration_seconds{quantile="0.95"} 0.450
```

**2. Labels for Dimensionality**:

```promql
# Bad: Single metric per endpoint
api_endpoint_users_requests_total
api_endpoint_posts_requests_total
api_endpoint_comments_requests_total

# Good: Single metric with labels
http_requests_total{endpoint="/users", method="GET", status="200"}
http_requests_total{endpoint="/posts", method="POST", status="201"}
http_requests_total{endpoint="/comments", method="GET", status="200"}

# Now you can query flexibly:
sum(rate(http_requests_total[5m])) by (endpoint)  # Per endpoint
sum(rate(http_requests_total{status=~"5.."}[5m]))  # All errors
```

**3. PromQL (Prometheus Query Language)**:

```promql
# Instant vector: Current value
node_memory_usage_bytes

# Range vector: Values over time window
node_memory_usage_bytes[5m]

# Rate: Per-second average increase
rate(http_requests_total[5m])

# Aggregation: Summarize across dimensions
sum by (service) (rate(http_requests_total[5m]))

# Math operations
(
  sum(rate(http_requests_total{status=~"5.."}[5m]))
  /
  sum(rate(http_requests_total[5m]))
) * 100  # Error percentage
```

### Prometheus Configuration

**prometheus.yml**:
```yaml
global:
  scrape_interval: 15s        # How often to scrape targets
  evaluation_interval: 15s    # How often to evaluate rules
  external_labels:
    cluster: 'production'
    region: 'us-east-1'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']

# Alert rule files
rule_files:
  - 'alerts/*.yml'

# Scrape configurations (where to collect metrics)
scrape_configs:
  # Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Node exporter (system metrics)
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance

  # Application services
  - job_name: 'applications'
    static_configs:
      - targets:
          - 'api:8080'
          - 'worker:8081'
          - 'scheduler:8082'
    metrics_path: '/metrics'

  # PostgreSQL
  - job_name: 'postgresql'
    static_configs:
      - targets: ['postgres-exporter:9187']

  # Redis
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  # Docker containers (via cAdvisor)
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
```

### Alert Rules

**alerts/infrastructure.yml**:
```yaml
groups:
  - name: infrastructure_alerts
    interval: 30s
    rules:
      # High CPU usage
      - alert: HighCpuUsage
        expr: |
          100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value | humanize }}% (threshold: 80%)"

      # Memory pressure
      - alert: HighMemoryPressure
        expr: |
          (
            node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes
          ) / node_memory_MemTotal_bytes * 100 > 90
        for: 5m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "High memory pressure on {{ $labels.instance }}"
          description: "Memory usage is {{ $value | humanize }}% (threshold: 90%)"

      # Disk space
      - alert: DiskSpaceLow
        expr: |
          (
            node_filesystem_size_bytes{mountpoint="/"} -
            node_filesystem_free_bytes{mountpoint="/"}
          ) / node_filesystem_size_bytes{mountpoint="/"} * 100 > 85
        for: 5m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Disk space low on {{ $labels.instance }}"
          description: "Disk usage is {{ $value | humanize }}% (threshold: 85%)"

      # Service down
      - alert: ServiceDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
          component: availability
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.instance }} has been down for more than 2 minutes"

  - name: application_alerts
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
            /
            sum(rate(http_requests_total[5m])) by (service)
          ) * 100 > 5
        for: 3m
        labels:
          severity: critical
          component: application
        annotations:
          summary: "High error rate in {{ $labels.service }}"
          description: "Error rate is {{ $value | humanize }}% (threshold: 5%)"

      # Slow response time
      - alert: SlowResponseTime
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
          ) > 1.0
        for: 5m
        labels:
          severity: warning
          component: performance
        annotations:
          summary: "Slow response time in {{ $labels.service }}"
          description: "95th percentile latency is {{ $value | humanize }}s (threshold: 1s)"

      # Database connection pool exhaustion
      - alert: DatabaseConnectionPoolExhausted
        expr: |
          (
            postgresql_connections_active
            /
            postgresql_connections_max
          ) * 100 > 80
        for: 3m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "Pool utilization is {{ $value | humanize }}% (threshold: 80%)"
```

### Exporters: Instrumenting Everything

Prometheus scrapes metrics from **exporters** - small services that expose metrics in Prometheus format.

**Common exporters**:

| What to Monitor | Exporter | Metrics Port |
|-----------------|----------|--------------|
| Linux system | node_exporter | 9100 |
| Docker containers | cAdvisor | 8080 |
| PostgreSQL | postgres_exporter | 9187 |
| Redis | redis_exporter | 9121 |
| MongoDB | mongodb_exporter | 9216 |
| Nginx | nginx-prometheus-exporter | 9113 |
| HAProxy | haproxy_exporter | 9101 |
| Elasticsearch | elasticsearch_exporter | 9114 |

**Application instrumentation** (example in Go):
```go
package main

import (
    "net/http"
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promhttp"
)

var (
    // Counter: Total requests
    requestsTotal = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Total HTTP requests",
        },
        []string{"method", "endpoint", "status"},
    )

    // Histogram: Request duration
    requestDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name: "http_request_duration_seconds",
            Help: "HTTP request duration in seconds",
            Buckets: []float64{.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5},
        },
        []string{"method", "endpoint"},
    )

    // Gauge: Active connections
    activeConnections = prometheus.NewGauge(
        prometheus.GaugeOpts{
            Name: "http_active_connections",
            Help: "Number of active HTTP connections",
        },
    )
)

func init() {
    // Register metrics
    prometheus.MustRegister(requestsTotal)
    prometheus.MustRegister(requestDuration)
    prometheus.MustRegister(activeConnections)
}

func main() {
    // Your application routes
    http.HandleFunc("/api/users", handleUsers)

    // Prometheus metrics endpoint
    http.Handle("/metrics", promhttp.Handler())

    http.ListenAndServe(":8080", nil)
}

func handleUsers(w http.ResponseWriter, r *http.Request) {
    timer := prometheus.NewTimer(requestDuration.WithLabelValues(r.Method, "/api/users"))
    defer timer.ObserveDuration()

    activeConnections.Inc()
    defer activeConnections.Dec()

    // ... your handler logic ...

    requestsTotal.WithLabelValues(r.Method, "/api/users", "200").Inc()
}
```

Now your application exposes metrics at `http://localhost:8080/metrics`:
```
# HELP http_requests_total Total HTTP requests
# TYPE http_requests_total counter
http_requests_total{method="GET",endpoint="/api/users",status="200"} 1523

# HELP http_request_duration_seconds HTTP request duration in seconds
# TYPE http_request_duration_seconds histogram
http_request_duration_seconds_bucket{method="GET",endpoint="/api/users",le="0.005"} 45
http_request_duration_seconds_bucket{method="GET",endpoint="/api/users",le="0.01"} 120
...
```

---

## Grafana: Visualization and Dashboards

Grafana turns Prometheus metrics into actionable visual insights.

### Dashboard Design Principles

**1. Hierarchical layout**: Top-down from overview to detail

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Overall System Health (one glance)             ‚îÇ
‚îÇ  üü¢ All systems operational                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Golden Signals ‚îÇ Infrastructure ‚îÇ Applications  ‚îÇ
‚îÇ ‚Ä¢ Latency      ‚îÇ ‚Ä¢ CPU          ‚îÇ ‚Ä¢ Error Rate  ‚îÇ
‚îÇ ‚Ä¢ Traffic      ‚îÇ ‚Ä¢ Memory       ‚îÇ ‚Ä¢ Throughput  ‚îÇ
‚îÇ ‚Ä¢ Errors       ‚îÇ ‚Ä¢ Disk         ‚îÇ ‚Ä¢ Dependencies‚îÇ
‚îÇ ‚Ä¢ Saturation   ‚îÇ ‚Ä¢ Network      ‚îÇ ‚Ä¢ Queues      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Detailed Time Series (drill-down)              ‚îÇ
‚îÇ  [Graphs for investigation]                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**2. Use color meaningfully**:
- üü¢ Green: Good (< threshold)
- üü° Yellow: Warning (approaching threshold)
- üî¥ Red: Critical (exceeded threshold)
- ‚ö™ Gray: No data / not applicable

**3. Choose the right visualization**:

| Data Type | Best Visualization | Example |
|-----------|-------------------|---------|
| Single value now | Stat panel | Current CPU usage |
| Value over time | Time series graph | Request rate history |
| Multiple values | Bar gauge | Memory per service |
| Proportions | Pie chart | Disk usage by volume |
| Relationships | Heatmap | Latency distribution |
| Status | State timeline | Service up/down history |

### Example Dashboard JSON (Golden Signals)

**Grafana dashboard configuration**:
```json
{
  "dashboard": {
    "title": "Golden Signals - API Service",
    "panels": [
      {
        "title": "Latency (p95)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{service=\"api\"}[5m])) by (le))",
            "legendFormat": "p95 latency"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "thresholds": {
              "steps": [
                { "color": "green", "value": 0 },
                { "color": "yellow", "value": 0.2 },
                { "color": "red", "value": 0.5 }
              ]
            }
          }
        }
      },
      {
        "title": "Traffic (requests/sec)",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{service=\"api\"}[5m]))",
            "legendFormat": "total"
          },
          {
            "expr": "sum(rate(http_requests_total{service=\"api\"}[5m])) by (endpoint)",
            "legendFormat": "{{endpoint}}"
          }
        ]
      },
      {
        "title": "Errors (%)",
        "type": "graph",
        "targets": [
          {
            "expr": "(sum(rate(http_requests_total{service=\"api\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{service=\"api\"}[5m]))) * 100",
            "legendFormat": "error rate"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                { "color": "green", "value": 0 },
                { "color": "yellow", "value": 1 },
                { "color": "red", "value": 5 }
              ]
            }
          }
        }
      },
      {
        "title": "Saturation",
        "type": "gauge",
        "targets": [
          {
            "expr": "100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU"
          },
          {
            "expr": "(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100",
            "legendFormat": "Memory"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                { "color": "green", "value": 0 },
                { "color": "yellow", "value": 70 },
                { "color": "red", "value": 90 }
              ]
            },
            "max": 100
          }
        }
      }
    ]
  }
}
```

### Dashboard Provisioning

Instead of creating dashboards manually, provision them as code:

**grafana/provisioning/dashboards/dashboards.yml**:
```yaml
apiVersion: 1

providers:
  - name: 'default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    editable: true
    options:
      path: /etc/grafana/provisioning/dashboards/json
```

**grafana/provisioning/datasources/prometheus.yml**:
```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: false
    jsonData:
      timeInterval: 15s
```

Now place dashboard JSON files in `/etc/grafana/provisioning/dashboards/json/` and they'll be automatically loaded.

---

## Loki: Log Aggregation

Loki is like Prometheus for logs - efficient, scalable log aggregation without indexing every field.

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  Your Services                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇService A ‚îÇ  ‚îÇService B ‚îÇ  ‚îÇService C ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  stdout  ‚îÇ  ‚îÇ  stdout  ‚îÇ  ‚îÇ  stdout  ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ             ‚îÇ             ‚îÇ
        ‚îÇ  Docker log driver         ‚îÇ
        ‚îÇ             ‚îÇ             ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ     Promtail   ‚îÇ ‚Üê Log collector
              ‚îÇ  (Agent on     ‚îÇ ‚Üê Adds labels
              ‚îÇ   each node)   ‚îÇ ‚Üê Pushes to Loki
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ      Loki      ‚îÇ
              ‚îÇ   (Storage)    ‚îÇ ‚Üê Stores logs with labels
              ‚îÇ                ‚îÇ ‚Üê Query with LogQL
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ    Grafana     ‚îÇ
              ‚îÇ  (Explore &    ‚îÇ ‚Üê View logs
              ‚îÇ   Dashboards)  ‚îÇ ‚Üê Create log panels
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Concepts

**1. Labels for indexing, not content**:

Loki only indexes labels (like Prometheus), not log content. This makes it extremely efficient.

```yaml
# Good: Structured labels
{
  job="api",
  environment="production",
  level="error",
  service="user-auth"
}

# Bad: High-cardinality labels
{
  user_id="12345",          # Too many unique values!
  request_id="abc-123",     # Creates too many streams
  ip_address="192.168.1.1"  # Not good for labels
}
```

**Rule**: Keep label cardinality low (<100 unique values per label).

**2. Log streams**:

A stream is a unique combination of labels:
```
{job="api", level="info"}   ‚Üí Stream 1
{job="api", level="error"}  ‚Üí Stream 2
{job="worker", level="info"} ‚Üí Stream 3
```

**3. LogQL (Loki Query Language)**:

Similar to PromQL but for logs:

```logql
# All logs from API service
{job="api"}

# Only error logs
{job="api", level="error"}

# Logs containing "timeout"
{job="api"} |= "timeout"

# Logs NOT containing "health check"
{job="api"} != "health check"

# Parse JSON and filter
{job="api"} | json | user_id="12345"

# Count error logs per minute
sum(count_over_time({job="api", level="error"}[1m]))

# Rate of specific error
rate({job="api"} |= "database connection failed" [5m])
```

### Promtail Configuration

**promtail-config.yml**:
```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # Docker containers
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      # Container name as label
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.*)'
        target_label: 'container'

      # Container labels as Loki labels
      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']
        target_label: 'service'

      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']
        target_label: 'project'

  # System logs
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: syslog
          __path__: /var/log/syslog

  # Application logs
  - job_name: applications
    static_configs:
      - targets:
          - localhost
        labels:
          job: applications
          __path__: /var/log/apps/*.log
    pipeline_stages:
      # Parse JSON logs
      - json:
          expressions:
            level: level
            message: message
            user_id: user_id
            request_id: request_id

      # Extract labels
      - labels:
          level:
          user_id:

      # Drop health check logs
      - drop:
          source: message
          expression: ".*health.*"
```

### Structured Logging Best Practices

To get the most from Loki, emit structured logs:

**Good (JSON structured)**:
```json
{
  "timestamp": "2024-03-15T10:23:45.123Z",
  "level": "error",
  "service": "api",
  "message": "Database query failed",
  "error": "connection timeout",
  "query": "SELECT * FROM users WHERE id = $1",
  "duration_ms": 30000,
  "user_id": "12345",
  "request_id": "abc-def-123",
  "trace_id": "xyz-789"
}
```

**Bad (unstructured)**:
```
2024-03-15 10:23:45 ERROR: Database query failed: connection timeout (user 12345)
```

**Why structured is better**:
- Easy to parse specific fields
- Can query by exact values
- Correlate with traces and metrics
- Machine-readable for automation

---

## Jaeger: Distributed Tracing

Tracing shows the journey of a request through your distributed system.

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Your Distributed Services            ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   API   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Auth   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇDatabase ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Gateway ‚îÇ    ‚îÇ Service ‚îÇ    ‚îÇ         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ       ‚îÇ              ‚îÇ                         ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ              ‚îÇ                                  ‚îÇ
‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ       ‚îÇ   Payment   ‚îÇ                          ‚îÇ
‚îÇ       ‚îÇ   Service   ‚îÇ                          ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  Each service sends trace spans ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ Jaeger Collector ‚îÇ
                                    ‚îÇ  (Receives spans)‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ Jaeger Storage   ‚îÇ
                                    ‚îÇ (Elasticsearch/  ‚îÇ
                                    ‚îÇ  Cassandra/etc)  ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ  Jaeger Query    ‚îÇ
                                    ‚îÇ    & UI          ‚îÇ
                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Concepts

**1. Trace**: Complete journey of a request
```
Trace ID: abc123

‚îî‚îÄ Span: API Gateway (100ms)
   ‚îú‚îÄ Span: Auth Service (30ms)
   ‚îÇ  ‚îî‚îÄ Span: Database Query (20ms)
   ‚îî‚îÄ Span: Payment Service (50ms)
      ‚îî‚îÄ Span: External API Call (40ms)
```

**2. Span**: A single operation within a trace
```yaml
span:
  trace_id: "abc123"
  span_id: "span456"
  parent_span_id: "span123"  # Links to parent
  operation_name: "POST /api/orders"
  start_time: 1647349425.123
  duration: 0.050  # 50ms
  tags:
    http.method: "POST"
    http.url: "/api/orders"
    http.status_code: 200
    service: "api"
  logs:
    - timestamp: 1647349425.130
      message: "Validating order"
    - timestamp: 1647349425.145
      message: "Order validated successfully"
```

**3. Context propagation**:

Trace context must be passed between services:

```
HTTP Request Headers:
  uber-trace-id: abc123:span456:0:1

gRPC Metadata:
  trace-id: abc123
  span-id: span456
  parent-span-id: span123
```

### Instrumenting Applications

**Example (Go with OpenTelemetry)**:
```go
package main

import (
    "context"
    "net/http"

    "go.opentelemetry.io/otel"
    "go.opentelemetry.io/otel/exporters/jaeger"
    "go.opentelemetry.io/otel/sdk/trace"
    "go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp"
)

func main() {
    // Initialize Jaeger exporter
    exporter, _ := jaeger.New(
        jaeger.WithCollectorEndpoint(
            jaeger.WithEndpoint("http://jaeger:14268/api/traces"),
        ),
    )

    // Create trace provider
    tp := trace.NewTracerProvider(
        trace.WithBatcher(exporter),
        trace.WithResource(resource.NewWithAttributes(
            semconv.ServiceNameKey.String("api"),
        )),
    )
    otel.SetTracerProvider(tp)

    // Instrument HTTP handlers
    http.Handle("/api/orders",
        otelhttp.NewHandler(
            http.HandlerFunc(handleOrders),
            "POST /api/orders",
        ),
    )

    http.ListenAndServe(":8080", nil)
}

func handleOrders(w http.ResponseWriter, r *http.Request) {
    ctx := r.Context()
    tracer := otel.Tracer("api")

    // Start a span
    ctx, span := tracer.Start(ctx, "handleOrders")
    defer span.End()

    // Add attributes
    span.SetAttributes(
        attribute.String("user.id", "12345"),
        attribute.Int("order.items", 3),
    )

    // Call downstream service (context propagates automatically)
    if err := callAuthService(ctx); err != nil {
        span.RecordError(err)
        http.Error(w, "auth failed", http.StatusUnauthorized)
        return
    }

    // Process order...

    w.WriteHeader(http.StatusCreated)
}

func callAuthService(ctx context.Context) error {
    tracer := otel.Tracer("api")

    // Create child span
    ctx, span := tracer.Start(ctx, "callAuthService")
    defer span.End()

    // HTTP client with tracing
    client := &http.Client{Transport: otelhttp.NewTransport(http.DefaultTransport)}

    req, _ := http.NewRequestWithContext(ctx, "GET", "http://auth:8081/validate", nil)
    resp, err := client.Do(req)  // Trace context propagated automatically

    if err != nil {
        return err
    }
    defer resp.Body.Close()

    return nil
}
```

### Using Jaeger for Troubleshooting

**Scenario**: User reports slow checkout

1. **Find the trace**:
   - Search by operation: `POST /api/checkout`
   - Filter by latency: `> 2s`
   - Filter by status: `error`

2. **Analyze the waterfall**:
```
Trace: abc123 (Total: 2.5s)
‚îÇ
‚îú‚îÄ API Gateway: 2.5s ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
‚îÇ  ‚îú‚îÄ Auth Service: 0.05s ‚ñè
‚îÇ  ‚îú‚îÄ Cart Service: 0.1s ‚ñé
‚îÇ  ‚îî‚îÄ Payment Service: 2.3s ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
‚îÇ     ‚îî‚îÄ External Payment API: 2.2s ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
```

3. **Identify bottleneck**: External payment API taking 2.2s (88% of total time)

4. **Root cause**: Payment service timeout set too high (30s), slow third-party API

5. **Solution**:
   - Reduce timeout to 5s
   - Add circuit breaker
   - Cache payment validation for 5 minutes
   - Add fallback payment provider

**This is observability in action**: Metrics alerted on slow checkout, logs showed payment errors, traces revealed the exact bottleneck.

---

## Integration: Correlating Metrics, Logs, and Traces

The power of observability comes from connecting all three pillars.

### Correlation Strategy

**1. Shared identifiers**:
```yaml
# Every request gets:
trace_id: "abc-123"       # Unique per request
span_id: "def-456"        # Unique per operation
request_id: "ghi-789"     # Unique per user request

# Include in:
- Trace spans (automatically)
- Log messages (add to structured logs)
- Metric labels (where cardinality allows)
```

**2. Example: Correlated observability data**:

**Metrics** (Prometheus):
```promql
http_request_duration_seconds{
  service="api",
  endpoint="/checkout",
  status="500"
} = 2.5
```
*Alert fires: API checkout latency > 2s*

**Logs** (Loki):
```json
{
  "timestamp": "2024-03-15T10:23:45.123Z",
  "level": "error",
  "service": "payment",
  "message": "Payment processing timeout",
  "error": "context deadline exceeded",
  "trace_id": "abc-123",      ‚Üê Correlation key
  "span_id": "def-456",
  "request_id": "ghi-789",
  "user_id": "12345"
}
```
*Search logs by trace_id to see full story*

**Traces** (Jaeger):
```
Trace ID: abc-123           ‚Üê Correlation key

API Gateway (2.5s)
  ‚îú‚îÄ Auth (0.05s)
  ‚îú‚îÄ Cart (0.1s)
  ‚îî‚îÄ Payment (2.3s) ‚ö†Ô∏è SLOW
     ‚îî‚îÄ External API (2.2s) ‚ö†Ô∏è BOTTLENECK
```
*Visualize exact flow and bottleneck*

### Grafana Integrated View

Create a Grafana dashboard with all three:

```json
{
  "panels": [
    {
      "title": "Request Latency (Metrics)",
      "type": "graph",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
        }
      ]
    },
    {
      "title": "Error Logs (Logs)",
      "type": "logs",
      "datasource": "Loki",
      "targets": [
        {
          "expr": "{service=\"$service\", level=\"error\"}"
        }
      ]
    },
    {
      "title": "Traces (Links)",
      "type": "table",
      "datasource": "Jaeger",
      "targets": [
        {
          "query": "service=$service AND error=true",
          "limit": 20
        }
      ],
      "links": [
        {
          "title": "View Trace",
          "url": "http://jaeger:16686/trace/${__data.fields.traceID}"
        }
      ]
    }
  ]
}
```

**Workflow**:
1. **Metrics alert**: "API latency spike detected"
2. **Click annotation** ‚Üí Opens Grafana at alert time
3. **View logs panel** ‚Üí See error messages with trace IDs
4. **Click trace ID link** ‚Üí Opens Jaeger with full trace
5. **Analyze bottleneck** ‚Üí Identify root cause
6. **Fix and deploy** ‚Üí Verify with metrics

**Time to resolution**: Minutes instead of hours.

---

## Real-World Implementation: Complete Observability Stack

Let's build a production-ready observability stack using agentic engineering from Chapter 3.

### Scenario

You're running a microservices architecture:
- 5 services (API Gateway, Auth, Users, Products, Orders)
- PostgreSQL database
- Redis cache
- Nginx load balancer
- Running on a VPS with 16GB RAM, 8 CPU cores

**Goal**: Implement complete observability with metrics, logs, and traces.

### Implementation Strategy

We'll build this in phases:
1. **Metrics** (Prometheus + Grafana + Exporters)
2. **Logs** (Loki + Promtail)
3. **Traces** (Jaeger)
4. **Integration** (Correlate all three)

### Phase 1: Metrics Stack

**Prompt to AI**:
```markdown
I need to set up Prometheus and Grafana for monitoring my microservices.

**Context**:
- Ubuntu 22.04 VPS
- Docker Compose v2
- 16GB RAM, 8 CPU cores, 200GB disk
- Existing services: 5 microservices + PostgreSQL + Redis + Nginx

**Requirements**:
1. Prometheus for metrics collection
2. Grafana for visualization
3. Node exporter for system metrics
4. cAdvisor for container metrics
5. PostgreSQL exporter
6. Redis exporter
7. Alertmanager for alerting

**Specifications**:
- 60-day metric retention
- 15-second scrape interval
- Resource limits appropriate for 16GB system
- Golden Signals dashboard pre-configured
- Alerts for CPU > 80%, Memory > 85%, Disk > 90%
- Alerts to Slack webhook: https://hooks.slack.com/services/xxx

**Provide**:
1. Complete docker-compose.yml with all services
2. Prometheus configuration with scrape configs
3. Alert rules for infrastructure and applications
4. Grafana dashboard JSON for Golden Signals
5. Deployment and validation scripts
```

**Expected AI output**: Complete metrics stack configuration (similar to Chapter 3 example but with 60-day retention and more services).

### Phase 2: Log Aggregation

**Prompt to AI**:
```markdown
Add Loki and Promtail to my existing monitoring stack for log aggregation.

**Current setup**:
[Paste docker-compose.yml from Phase 1]

**Requirements**:
1. Loki for log storage
2. Promtail to collect logs from all Docker containers
3. Retain logs for 30 days
4. Integrate with existing Grafana
5. Configure Grafana Explore for log searching
6. Create log dashboard showing:
   - Error rate over time
   - Top 10 error messages
   - Logs by service
   - Correlation with metrics

**Specifications**:
- Parse JSON logs from applications
- Extract level, service, message, trace_id as labels
- Drop health check logs to reduce volume
- Resource limits: Max 4GB RAM for Loki

**Provide**:
1. Updated docker-compose.yml (add Loki + Promtail)
2. Promtail configuration for Docker log collection
3. Loki configuration with retention
4. Grafana datasource configuration for Loki
5. Example log dashboard JSON
```

**Expected output**: Integrated log aggregation with metric correlation.

### Phase 3: Distributed Tracing

**Prompt to AI**:
```markdown
Add Jaeger to my observability stack for distributed tracing.

**Current setup**:
[Paste docker-compose.yml from Phase 2]

**Requirements**:
1. Jaeger all-in-one (collector, query, UI)
2. OpenTelemetry Collector for trace ingestion
3. Store traces for 7 days
4. Integrate with Grafana
5. Configure trace sampling:
   - 100% of errors
   - 10% of successful requests
   - 100% of requests > 1s

**Specifications**:
- Use Elasticsearch backend for trace storage
- Expose OTLP endpoints (HTTP: 4318, gRPC: 4317)
- Resource limits: Max 2GB RAM for Jaeger

**Application instrumentation guidance**:
For each service, provide example code to:
1. Initialize OpenTelemetry SDK
2. Create and propagate trace context
3. Add custom span attributes
4. Link traces to logs (inject trace_id)

**Provide**:
1. Updated docker-compose.yml (add Jaeger + OTEL Collector + Elasticsearch)
2. OpenTelemetry Collector configuration
3. Jaeger configuration
4. Example instrumentation code (Go, Python, Node.js)
5. Grafana datasource configuration for Jaeger
```

**Expected output**: Complete tracing implementation with instrumentation examples.

### Phase 4: Integration and Correlation

**Prompt to AI**:
```markdown
Create an integrated Grafana dashboard that correlates metrics, logs, and traces.

**Current stack**:
- Prometheus (metrics)
- Loki (logs)
- Jaeger (traces)

**Dashboard requirements**:

**Top row** (Overview):
- System health stat (green/yellow/red based on alerts)
- Total requests/sec (gauge)
- Error rate % (stat with threshold coloring)
- P95 latency (stat with trend)

**Second row** (Golden Signals):
- Latency graph (p50, p95, p99)
- Traffic graph (requests/sec by service)
- Errors graph (error rate by service)
- Saturation gauges (CPU, Memory, Disk)

**Third row** (Logs and Traces):
- Error logs panel (last 100 errors)
  - Clickable trace IDs that link to Jaeger
- Recent slow traces table (>1s)
  - Links to Jaeger trace viewer

**Fourth row** (Service Details):
- Requests/sec per service
- Error rate per service
- Latency per service
- Active connections per service

**Features**:
- Time range selector
- Service variable dropdown
- Auto-refresh every 30s
- Annotations for deployments
- Alert state indicators

**Provide**:
1. Complete Grafana dashboard JSON
2. Variable definitions
3. Panel linking configuration
4. Explanation of key queries
```

**Expected output**: Production-ready integrated dashboard.

### Final Result

After these four phases, you have:

‚úÖ **Metrics** (Prometheus):
- 60-day retention
- Scraping 8+ targets every 15s
- 20+ alert rules
- Resource-efficient storage

‚úÖ **Logs** (Loki):
- 30-day retention
- Collecting logs from all containers
- Structured JSON parsing
- <1GB/day storage

‚úÖ **Traces** (Jaeger):
- 7-day retention
- Distributed request tracing
- Elasticsearch backend
- Sampling configuration

‚úÖ **Visualization** (Grafana):
- Golden Signals dashboard
- Integrated metrics + logs + traces
- Drill-down capabilities
- Alert annotations

‚úÖ **Alerting** (Alertmanager):
- Infrastructure alerts
- Application alerts
- Slack integration
- Alert grouping and routing

**Total implementation time**:
- Manual: ~40 hours
- With AI: ~6 hours
- **Time saved: 34 hours** üéâ

**Resource usage**:
- Prometheus: 4GB RAM
- Loki: 2GB RAM
- Jaeger: 2GB RAM
- Elasticsearch: 4GB RAM
- Grafana: 1GB RAM
- Exporters: 1GB RAM
- **Total: 14GB / 16GB** (88% utilized, 2GB headroom)

---

## Best Practices for Observability

### Metrics Best Practices

**DO**:
‚úÖ Use labels for dimensions, not separate metrics
‚úÖ Keep label cardinality low (<100 unique values per label)
‚úÖ Use histograms for latency, not averages
‚úÖ Record error rates, not just error counts
‚úÖ Implement the Golden Signals for every service
‚úÖ Set alerts based on SLOs, not arbitrary thresholds

**DON'T**:
‚ùå Don't use high-cardinality labels (user IDs, IP addresses)
‚ùå Don't create metrics for every endpoint (use labels)
‚ùå Don't rely on averages (they hide problems)
‚ùå Don't over-alert (alert fatigue is real)
‚ùå Don't forget to test your alerts (simulate failures)

### Logging Best Practices

**DO**:
‚úÖ Use structured logging (JSON)
‚úÖ Include trace_id and span_id in every log
‚úÖ Log at appropriate levels (DEBUG, INFO, WARN, ERROR)
‚úÖ Include context (user_id, request_id, service name)
‚úÖ Log business events, not just errors
‚úÖ Aggregate logs centrally (don't SSH to servers)

**DON'T**:
‚ùå Don't log sensitive data (passwords, tokens, PII)
‚ùå Don't log too much (disk fills up fast)
‚ùå Don't use unstructured text logs
‚ùå Don't mix log levels arbitrarily
‚ùå Don't forget to rotate logs

### Tracing Best Practices

**DO**:
‚úÖ Instrument all inter-service calls
‚úÖ Propagate trace context across boundaries
‚úÖ Sample intelligently (100% errors, sample successes)
‚úÖ Add custom attributes for business context
‚úÖ Link traces to logs and metrics
‚úÖ Set appropriate timeouts and deadlines

**DON'T**:
‚ùå Don't trace every function (overhead!)
‚ùå Don't ignore sampling (costs add up)
‚ùå Don't forget to propagate context
‚ùå Don't add high-cardinality tags
‚ùå Don't store traces forever (expensive)

### Dashboard Best Practices

**DO**:
‚úÖ Design for humans, not machines
‚úÖ Use hierarchical layouts (overview ‚Üí detail)
‚úÖ Apply color meaningfully (green/yellow/red)
‚úÖ Choose appropriate visualizations
‚úÖ Include comparison periods (vs. yesterday, vs. last week)
‚úÖ Add context with annotations

**DON'T**:
‚ùå Don't create information overload
‚ùå Don't use pie charts for time-series
‚ùå Don't ignore accessibility (color-blind users)
‚ùå Don't forget mobile responsiveness
‚ùå Don't create dashboards nobody looks at

---

## Troubleshooting with Observability

### Scenario 1: Service is Slow

**Step 1: Check metrics**
```promql
# Is latency actually elevated?
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{service="api"}[5m]))

# Which endpoints are slow?
topk(5, histogram_quantile(0.95,
  rate(http_request_duration_seconds_bucket{service="api"}[5m]) by (endpoint)
))

# Is it correlated with traffic?
rate(http_requests_total{service="api"}[5m])
```

**Step 2: Check saturation**
```promql
# CPU usage
100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory pressure
(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100

# Connection pool saturation
postgresql_connections_active / postgresql_connections_max * 100
```

**Step 3: Examine logs**
```logql
# Recent errors
{service="api", level="error"} [5m]

# Slow query logs
{service="api"} |= "slow query" | json | duration > 1000
```

**Step 4: Analyze traces**
- Find slow traces (> 2s)
- Look at waterfall view
- Identify bottleneck span
- Check span attributes for details

**Resolution**: Increase database connection pool from 10 to 20, add query index.

### Scenario 2: Increased Error Rate

**Step 1: Quantify the problem**
```promql
# Error rate
sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) * 100

# Which services?
sum by (service) (rate(http_requests_total{status=~"5.."}[5m]))

# Which endpoints?
topk(10, sum by (endpoint) (rate(http_requests_total{status=~"5.."}[5m])))
```

**Step 2: Find error details in logs**
```logql
# Error messages
{level="error"} [10m]

# Group by error type
{level="error"} | json | count by (error_type) over [10m]
```

**Step 3: Trace a failing request**
- Search Jaeger for recent error traces
- Examine which span failed
- Check span tags and logs
- Correlate with service logs using trace_id

**Resolution**: External API timeout, add circuit breaker and fallback.

### Scenario 3: Mysterious Memory Leak

**Step 1: Confirm with metrics**
```promql
# Memory usage trend
container_memory_usage_bytes{container="api"}

# Memory growth rate
rate(container_memory_usage_bytes{container="api"}[1h])
```

**Step 2: Check for resource leaks**
```promql
# Database connections not released?
postgresql_connections_active

# Go heap objects growing?
go_memstats_heap_objects

# Cache size unbounded?
redis_memory_usage_bytes
```

**Step 3: Correlate with logs**
```logql
# OOM events
{service="api"} |= "out of memory"

# Memory warnings
{service="api", level="warn"} |= "memory"
```

**Step 4: Profile with traces**
- Look for accumulating spans
- Check for long-lived operations
- Identify leaked goroutines/threads

**Resolution**: Database connection not closed in error path, added defer Close().

---

## Chapter Summary

You've learned how to implement comprehensive observability for your infrastructure:

**Core Concepts**:
- **Three pillars**: Metrics (what), Logs (why), Traces (where)
- **Golden Signals**: Latency, Traffic, Errors, Saturation
- **Integration**: Correlating data across pillars for rapid troubleshooting

**Practical Skills**:
- **Prometheus**: Metrics collection, PromQL queries, alert rules
- **Grafana**: Dashboard design, visualization, data source integration
- **Loki**: Log aggregation, LogQL queries, structured logging
- **Jaeger**: Distributed tracing, instrumentation, trace analysis

**Production Practices**:
- Design dashboards for humans (hierarchical, colored, contextual)
- Implement intelligent sampling (100% errors, sample successes)
- Correlate with shared identifiers (trace_id, request_id)
- Alert on SLOs, not arbitrary thresholds
- Build for debugging, not just monitoring

**Troubleshooting Workflow**:
1. **Metrics** ‚Üí Identify WHAT is wrong (latency spike, error rate increase)
2. **Logs** ‚Üí Understand WHY it's wrong (error messages, stack traces)
3. **Traces** ‚Üí Discover WHERE it's wrong (bottleneck service, slow operation)
4. **Fix** ‚Üí Deploy solution with confidence
5. **Verify** ‚Üí Confirm fix with observability data

**Key Takeaway**: Observability is not about collecting data‚Äîit's about gaining insight. The best observability stack is one that helps you understand your system's behavior and troubleshoot problems rapidly.

---

## What's Next: Chapter 5

In Chapter 5, we'll build **The Circulatory System (Networking)**: Traefik, Consul, NATS, and service mesh patterns.

You'll learn:
- How to implement intelligent request routing with Traefik
- Service discovery and health checking with Consul
- Event-driven architecture with NATS
- Load balancing strategies for high availability
- TLS termination and automatic certificate management
- API gateway patterns for microservices

**Preview**: We'll design and implement a complete networking layer that intelligently routes traffic, discovers services dynamically, and scales automatically‚Äîall while maintaining security and observability.

Ready to wire up your infrastructure's circulatory system? Turn to Chapter 5.

---

## Apex Observability Practices

Model your nervous system after the operators who run the largest fleets on the planet:

- **Unified Instrumentation** ‚Äî Standardize on OpenTelemetry SDKs/collectors with semantic conventions, exemplars, and baggage propagation so metrics, logs, and traces share identifiers automatically.
- **Planet-Scale Metrics** ‚Äî Use Prometheus paired with VictoriaMetrics, Thanos, or Grafana Mimir for durable long-term storage, native histogram support, and multi-tenancy without cardinality explosions.
- **Log Intelligence** ‚Äî Ship logs through Vector or Fluent Bit into Loki, Elastic, or ClickHouse with dynamic sampling, shredding of PII via Open Policy Agent, and retention tiers managed by S3/Glacier lifecycle policies.
- **Trace-Driven Debugging** ‚Äî Capture critical paths with Tempo or Jaeger plus eBPF-powered auto-instrumentation from Pixie or Parca to uncover kernel-level latency and resource contention.
- **SLO Automation** ‚Äî Define SLOs with Sloth or Nobl9, emit error budgets into PagerDuty/FireHydrant, and feed burn-rate alerts back into GitOps workflows for automated mitigation.
- **User-Centric Telemetry** ‚Äî Blend backend signals with Real User Monitoring (SpeedCurve, Akamai mPulse) and Synthetic probes (Checkly, Grafana k6) to understand perceived performance end-to-end.
- **Self-Healing Feedback Loops** ‚Äî Close the loop via Keptn or Autopilot pipelines that run diagnostics, recommend rollbacks, and enrich incidents with correlated traces/logs before humans arrive.

These practices deliver the same clarity and reflexes expected in hyperscale environments, even when you're running a single cluster.
